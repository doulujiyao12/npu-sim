import re
import statistics
from pathlib import Path
import csv
import json

def parse_data_file(file_path):
    """
    è§£æåŒ…å«æ—¶é—´æ•°æ®çš„æ–‡æœ¬æ–‡ä»¶
    
    Args:
        file_path (str): è¾“å…¥æ–‡ä»¶è·¯å¾„
        
    Returns:
        dict: è§£æåçš„æ•°æ®ï¼ŒæŒ‰ç»„ç»‡ç»“æ„ç»„ç»‡
    """
    with open(file_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åˆ†å‰²æ•°æ®ç»„
    groups = re.split(r'\*([^*]+)\*', content)
    
    parsed_data = {}
    
    for i in range(1, len(groups), 2):
        if i + 1 >= len(groups):
            break
            
        group_title = groups[i].strip()
        group_content = groups[i + 1].strip()
        
        if not group_content:
            continue
            
        requests = parse_requests(group_content)
        if requests:
            parsed_data[group_title] = requests
    
    return parsed_data

def parse_requests(content):
    """
    è§£æå•ä¸ªç»„å†…çš„è¯·æ±‚æ•°æ®
    
    Args:
        content (str): ç»„å†…å®¹æ–‡æœ¬
        
    Returns:
        list: è¯·æ±‚æ•°æ®åˆ—è¡¨
    """
    requests = []
    
    # æŒ‰è¯·æ±‚åˆ†å‰²
    request_sections = re.split(r'Request (\d+):', content)
    
    for i in range(1, len(request_sections), 2):
        if i + 1 >= len(request_sections):
            break
            
        request_id = int(request_sections[i])
        request_content = request_sections[i + 1].strip()
        
        # æå–tokenæ—¶é—´
        token_times = []
        token_pattern = r'Token (\d+): ([\d.]+)'
        matches = re.findall(token_pattern, request_content)
        
        for token_id, time_str in matches:
            token_times.append({
                'token_id': int(token_id),
                'time': float(time_str)
            })
        
        if token_times:
            requests.append({
                'request_id': request_id,
                'tokens': token_times
            })
    
    return requests

def calculate_metrics(group_data):
    """
    è®¡ç®—TTFTã€TBTã€å¹³å‡latencyå’Œthroughput
    
    Args:
        group_data (list): å•ç»„çš„è¯·æ±‚æ•°æ®
        
    Returns:
        dict: è®¡ç®—ç»“æœ
    """
    if not group_data:
        return None
    
    ttft_times = []  # Time to First Token
    tbt_times = []   # Time Between Tokens
    latency_times = []  # Total latency (time to last token)
    all_last_token_times = []  # æ‰€æœ‰è¯·æ±‚çš„æœ€åä¸€ä¸ªtokenæ—¶é—´
    total_tokens = 0  # æ€»tokenæ•°é‡
    
    for request in group_data:
        tokens = request['tokens']
        if not tokens:
            continue
            
        # æŒ‰token_idæ’åºç¡®ä¿é¡ºåºæ­£ç¡®
        tokens = sorted(tokens, key=lambda x: x['token_id'])
        
        # ç»Ÿè®¡tokenæ•°é‡
        total_tokens += len(tokens)
        
        # TTFT: ç¬¬0ä¸ªtokençš„æ—¶é—´ï¼ˆä»å¼€å§‹åˆ°ç¬¬ä¸€ä¸ªtokenï¼‰
        if tokens:
            ttft = tokens[0]['time']
            ttft_times.append(ttft)
        
        # Latency: æœ€åä¸€ä¸ªtokençš„æ—¶é—´
        if tokens:
            latency = tokens[-1]['time']
            latency_times.append(latency)
            all_last_token_times.append(latency)
        
        # TBT: ç›¸é‚»tokenä¹‹é—´çš„æ—¶é—´å·®
        for i in range(1, len(tokens)):
            tbt = tokens[i]['time'] - tokens[i-1]['time']
            tbt_times.append(tbt)
    
    # è®¡ç®—throughput: æœ€æ™šå®Œæˆçš„è¯·æ±‚æ—¶é—´ / æ€»tokenæ•°
    max_completion_time = max(all_last_token_times) if all_last_token_times else 0
    throughput_tokens_per_ns = total_tokens / statistics.mean(latency_times) 
    throughput_tokens_per_second = throughput_tokens_per_ns * 1e9  # è½¬æ¢ä¸ºæ¯ç§’tokenæ•°
    
    # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
    results = {
        'request_count': len(group_data),
        'token_count_per_request': len(group_data[0]['tokens']) if group_data else 0,
        'total_tokens': total_tokens,
        'max_completion_time_ns': max_completion_time,
        'throughput_tokens_per_second': throughput_tokens_per_second,
        'ttft': {
            'mean': statistics.mean(ttft_times) if ttft_times else 0,
            'median': statistics.median(ttft_times) if ttft_times else 0,
            'min': min(ttft_times) if ttft_times else 0,
            'max': max(ttft_times) if ttft_times else 0,
            'std': statistics.stdev(ttft_times) if len(ttft_times) > 1 else 0
        },
        'tbt': {
            'mean': statistics.mean(tbt_times) if tbt_times else 0,
            'median': statistics.median(tbt_times) if tbt_times else 0,
            'min': min(tbt_times) if tbt_times else 0,
            'max': max(tbt_times) if tbt_times else 0,
            'std': statistics.stdev(tbt_times) if len(tbt_times) > 1 else 0
        },
        'latency': {
            'mean': statistics.mean(latency_times) if latency_times else 0,
            'median': statistics.median(latency_times) if latency_times else 0,
            'min': min(latency_times) if latency_times else 0,
            'max': max(latency_times) if latency_times else 0,
            'std': statistics.stdev(latency_times) if len(latency_times) > 1 else 0
        }
    }
    
    return results

def format_time(nanoseconds):
    """
    å°†çº³ç§’è½¬æ¢ä¸ºæ›´æ˜“è¯»çš„æ ¼å¼
    """
    if nanoseconds >= 1e9:
        return f"{nanoseconds / 1e9:.3f} s"
    elif nanoseconds >= 1e6:
        return f"{nanoseconds / 1e6:.3f} ms"
    elif nanoseconds >= 1e3:
        return f"{nanoseconds / 1e3:.3f} Î¼s"
    else:
        return f"{nanoseconds:.3f} ns"

def save_results_to_csv(results, output_path):
    """
    å°†ç»“æœä¿å­˜ä¸ºCSVæ–‡ä»¶
    """
    with open(output_path, 'w', newline='', encoding='utf-8') as csvfile:
        fieldnames = [
            'Group', 'Request_Count', 'Token_Count_Per_Request', 'Total_Tokens', 
            'Max_Completion_Time_ns', 'Throughput_Tokens_Per_Second',
            'TTFT_Mean_ns', 'TTFT_Median_ns', 'TTFT_Min_ns', 'TTFT_Max_ns', 'TTFT_Std_ns',
            'TBT_Mean_ns', 'TBT_Median_ns', 'TBT_Min_ns', 'TBT_Max_ns', 'TBT_Std_ns',
            'Latency_Mean_ns', 'Latency_Median_ns', 'Latency_Min_ns', 'Latency_Max_ns', 'Latency_Std_ns',
            'TTFT_Mean_Formatted', 'TBT_Mean_Formatted', 'Latency_Mean_Formatted', 'Max_Completion_Time_Formatted'
        ]
        
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        writer.writeheader()
        
        for group_name, metrics in results.items():
            row = {
                'Group': group_name,
                'Request_Count': metrics['request_count'],
                'Token_Count_Per_Request': metrics['token_count_per_request'],
                'Total_Tokens': metrics['total_tokens'],
                'Max_Completion_Time_ns': metrics['max_completion_time_ns'],
                'Throughput_Tokens_Per_Second': f"{metrics['throughput_tokens_per_second']:.3f}",
                'TTFT_Mean_ns': metrics['ttft']['mean'],
                'TTFT_Median_ns': metrics['ttft']['median'],
                'TTFT_Min_ns': metrics['ttft']['min'],
                'TTFT_Max_ns': metrics['ttft']['max'],
                'TTFT_Std_ns': metrics['ttft']['std'],
                'TBT_Mean_ns': metrics['tbt']['mean'],
                'TBT_Median_ns': metrics['tbt']['median'],
                'TBT_Min_ns': metrics['tbt']['min'],
                'TBT_Max_ns': metrics['tbt']['max'],
                'TBT_Std_ns': metrics['tbt']['std'],
                'Latency_Mean_ns': metrics['latency']['mean'],
                'Latency_Median_ns': metrics['latency']['median'],
                'Latency_Min_ns': metrics['latency']['min'],
                'Latency_Max_ns': metrics['latency']['max'],
                'Latency_Std_ns': metrics['latency']['std'],
                'TTFT_Mean_Formatted': format_time(metrics['ttft']['mean']),
                'TBT_Mean_Formatted': format_time(metrics['tbt']['mean']),
                'Latency_Mean_Formatted': format_time(metrics['latency']['mean']),
                'Max_Completion_Time_Formatted': format_time(metrics['max_completion_time_ns'])
            }
            writer.writerow(row)

def save_results_to_json(results, output_path):
    """
    å°†ç»“æœä¿å­˜ä¸ºJSONæ–‡ä»¶
    """
    # ä¸ºæ¯ä¸ªç»„æ·»åŠ æ ¼å¼åŒ–çš„æ—¶é—´
    formatted_results = {}
    for group_name, metrics in results.items():
        formatted_metrics = metrics.copy()
        for metric_type in ['ttft', 'tbt', 'latency']:
            formatted_metrics[f'{metric_type}_formatted'] = {
                key: format_time(value) for key, value in metrics[metric_type].items()
            }
        # æ·»åŠ throughputç›¸å…³çš„æ ¼å¼åŒ–ä¿¡æ¯
        formatted_metrics['max_completion_time_formatted'] = format_time(metrics['max_completion_time_ns'])
        formatted_metrics['throughput_formatted'] = f"{metrics['throughput_tokens_per_second']:.3f} tokens/s"
        formatted_results[group_name] = formatted_metrics
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(formatted_results, f, indent=2, ensure_ascii=False)

def print_summary(results):
    """
    æ‰“å°åˆ†æç»“æœæ‘˜è¦
    """
    print("\n" + "="*80)
    print("æ•°æ®åˆ†æç»“æœæ‘˜è¦")
    print("="*80)
    
    for group_name, metrics in results.items():
        print(f"\nğŸ“Š ç»„: {group_name}")
        print(f"   è¯·æ±‚æ•°é‡: {metrics['request_count']}")
        print(f"   æ¯è¯·æ±‚tokenæ•°: {metrics['token_count_per_request']}")
        print(f"   æ€»tokenæ•°: {metrics['total_tokens']}")
        
        print(f"\n   ğŸš€ Throughput:")
        print(f"      ååé‡: {metrics['throughput_tokens_per_second']:.3f} tokens/ç§’")
        print(f"      æœ€æ™šå®Œæˆæ—¶é—´: {format_time(metrics['max_completion_time_ns'])}")
        
        print(f"\n   â±ï¸  TTFT (Time to First Token):")
        print(f"      å¹³å‡å€¼: {format_time(metrics['ttft']['mean'])}")
        print(f"      ä¸­ä½æ•°: {format_time(metrics['ttft']['median'])}")
        print(f"      æ ‡å‡†å·®: {format_time(metrics['ttft']['std'])}")
        
        print(f"\n   ğŸ”„ TBT (Time Between Tokens):")
        print(f"      å¹³å‡å€¼: {format_time(metrics['tbt']['mean'])}")
        print(f"      ä¸­ä½æ•°: {format_time(metrics['tbt']['median'])}")
        print(f"      æ ‡å‡†å·®: {format_time(metrics['tbt']['std'])}")
        
        print(f"\n   ğŸ“ˆ Latency (Total Time):")
        print(f"      å¹³å‡å€¼: {format_time(metrics['latency']['mean'])}")
        print(f"      ä¸­ä½æ•°: {format_time(metrics['latency']['median'])}")
        print(f"      æ ‡å‡†å·®: {format_time(metrics['latency']['std'])}")
        
        print("-" * 60)

def main():
    # è¾“å…¥æ–‡ä»¶è·¯å¾„
    input_file = input("è¯·è¾“å…¥æ•°æ®æ–‡ä»¶è·¯å¾„ (é»˜è®¤: data.txt): ").strip()
    if not input_file:
        input_file = "data.txt"
    input_file = "pdhete/" + input_file + ".txt"
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not Path(input_file).exists():
        print(f"é”™è¯¯: æ–‡ä»¶ '{input_file}' ä¸å­˜åœ¨!")
        return
    
    print(f"æ­£åœ¨åˆ†ææ–‡ä»¶: {input_file}")
    
    # è§£ææ•°æ®
    try:
        parsed_data = parse_data_file(input_file)
        print(f"æˆåŠŸè§£æ {len(parsed_data)} ä¸ªæ•°æ®ç»„")
    except Exception as e:
        print(f"è§£ææ–‡ä»¶æ—¶å‡ºé”™: {e}")
        return
    
    # è®¡ç®—æŒ‡æ ‡
    results = {}
    for group_name, group_data in parsed_data.items():
        print(f"æ­£åœ¨å¤„ç†ç»„: {group_name}")
        metrics = calculate_metrics(group_data)
        if metrics:
            results[group_name] = metrics
    
    if not results:
        print("æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„æ•°æ®è¿›è¡Œåˆ†æ!")
        return
    
    # æ‰“å°æ‘˜è¦
    print_summary(results)
    
    # ä¿å­˜ç»“æœ
    output_base = Path(input_file).stem
    
    # ä¿å­˜ä¸ºCSV
    # csv_output = f"{output_base}_analysis_results.csv"
    # save_results_to_csv(results, csv_output)
    # print(f"\nâœ… CSVç»“æœå·²ä¿å­˜åˆ°: {csv_output}")
    
    # ä¿å­˜ä¸ºJSON
    json_output = f"pdhete/{output_base}_analysis_results.json"
    save_results_to_json(results, json_output)
    print(f"âœ… JSONç»“æœå·²ä¿å­˜åˆ°: {json_output}")
    
    print(f"\nğŸ‰ åˆ†æå®Œæˆ! å…±å¤„ç†äº† {len(results)} ä¸ªæ•°æ®ç»„")

if __name__ == "__main__":
    main()